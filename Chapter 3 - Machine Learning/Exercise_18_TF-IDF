import numpy as np

text = '''Humpty Dumpty sat on a wall
Humpty Dumpty had a great fall
all the king's horses and all the king's men
couldn't put Humpty together again'''

def manhattan_distance(row1, row2):
    return sum(abs(np.asarray(row1) - np.asarray(row2)))


def permutations(size):
    for i in range(size):
        for j in range(i + 1, size):
            yield (i, j)

def main(text):
    # tasks your code should perform:

    # 1. split the text into words, and get a list of unique words that appear in it
    # a short one-liner to separate the text into sentences (with words lower-cased to make words equal 
    # despite casing) can be done with 
    docs = [line.lower().split() for line in text.split('\n')]

    unique_words=sorted(set(text.lower().split()))

    n_docs = len(docs)
    n_words = len(unique_words)

    print("Unique words:", unique_words)
    print("Number of documents:", n_docs)
    print("Number of unique words:", n_words)

    # 2. go over each unique word and calculate its term frequency, and its document frequency
    term_frequencies = np.empty((n_docs, n_words), dtype=float) #tf
    document_frequencies = np.empty(n_words, dtype=float) #df

    for i, word in enumerate(unique_words):
        term_frequencies[:, i] = [doc.count(word) / len(doc) for doc in docs]
        document_frequencies[i] = sum(1 for doc in docs if word in doc)

    print("Term Frequencies:", term_frequencies)
    print("Document Frequencies:", document_frequencies)

    # 3. after you have your term frequencies and document frequencies, go over each line in the text and 
    # calculate its TF-IDF representation, which will be a vector
    tf_idf_vector=np.zeros((n_docs, n_words), dtype=float)

    for i, doc in enumerate(docs):
        for j, word in enumerate(unique_words):
            tf = term_frequencies[i, j]
            df = n_docs / document_frequencies[j]
            tf_idf_vector[i, j] = tf * np.log(1 /df) # tf−idf = tf × log(1÷df)

    print("TF-IDF Vectors:")
    print(tf_idf_vector)

    # 4. after you have calculated the TF-IDF representations for each line in the text, you need to
    # calculate the distances between each line to find which are the closest.

    dist = np.full((n_docs, n_docs), np.inf, dtype=float)

    print(dist)

    for (i, j) in permutations(n_docs):
        print(f"Calculating distance between {i} and {j}")
        distance = manhattan_distance(tf_idf_vector[i], tf_idf_vector[j])
        dist[i, j] = distance
        dist[j, i] = distance

    print(dist)

    index = np.unravel_index(np.argmin(dist), dist.shape)
    print(index)


main(text)
